# GPT-2 Reimplementation

## üìå Project Overview
This repository contains a reimplementation of OpenAI's **GPT-2** model from scratch. The goal is to understand and reproduce the core functionalities of GPT-2, including **tokenization, transformer architecture, training, and inference**.

## üß† Understanding GPT-2
GPT-2 is an **autoregressive Transformer model** designed for **text generation**. It consists of:
- **Multi-layer Transformer blocks**
- **Self-attention for contextual word understanding**
- **Layer normalization and residual connections**
- **Token embeddings and positional encodings**
## Dataset
We use the FineWeb-Edu dataset from Hugging Face for training. This dataset consists of high-quality web text specifically filtered for educational purposes. You can find it here: [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)

## üìö References
- [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762)
- [OpenAI GPT-2 Blog](https://openai.com/research/gpt-2)
- [Andrej Karpathy](https://youtu.be/l8pRSuU81PU?si=G_lhGZRfgYW9NLJZ)
- [Hugging Face Transformers](https://huggingface.co/transformers/)

## üèÜ Contributors
- [efeecllk](https://github.com/efeecllk)
- [merttomekce](https://github.com/merttomekce)
 
