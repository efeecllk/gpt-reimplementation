# GPT-2 Reimplementation README

## ğŸ“Œ Project Overview
This repository contains a reimplementation of OpenAI's **GPT-2** model from scratch. The goal is to understand and reproduce the core functionalities of GPT-2, including **tokenization, transformer architecture, training, and inference**.

## ğŸ§  Understanding GPT-2
GPT-2 is an **autoregressive Transformer model** designed for **text generation**. It consists of:
- **Multi-layer Transformer blocks**
- **Self-attention for contextual word understanding**
- **Layer normalization and residual connections**
- **Token embeddings and positional encodings**

## ğŸ“š References
- [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762)
- [OpenAI GPT-2 Blog](https://openai.com/research/gpt-2)
- [Hugging Face Transformers](https://huggingface.co/transformers/)

## ğŸ† Contributors
- [efeecllk](https://github.com/efeecllk)
- [merttomekce](https://github.com/merttomekce)
 
